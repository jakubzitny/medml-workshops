{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ee5iuxsh0wlZ"
   },
   "source": [
    "# Basic datasets with CNNs and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "dIHNZp8vdpHG",
    "outputId": "a7b21e49-671b-41d6-becc-7c162aa8211f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "2.2.4-tf\n",
      "10.109.223.170:8470\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "print(os.environ['COLAB_TPU_ADDR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0iZwcOO0s75"
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rM7jkisLMHk0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "K1b6qe9hd8Pl",
    "outputId": "bb60fdc9-af3f-4b4d-d9df-cdbdd3476d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found TPU at: grpc://10.109.223.170:8470\n"
     ]
    }
   ],
   "source": [
    "# NOTE: TPU setup\n",
    "# SEE: https://medium.com/tensorflow/tf-keras-on-tpus-on-colab-674367932aa0\n",
    "\n",
    "try:\n",
    "  device_name = os.environ['COLAB_TPU_ADDR']\n",
    "  TPU_ADDRESS = 'grpc://' + device_name\n",
    "  print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
    "except KeyError:\n",
    "  print('TPU not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vgb4sqVbDUwn"
   },
   "source": [
    "### **CIFAR10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24mgFn4gLrYl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fy6Ncisq1q_"
   },
   "outputs": [],
   "source": [
    "(cifar_train, cifar_train_labels_sparse), (cifar_test, cifar_test_labels_sparse) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rok3es7FTZz9"
   },
   "outputs": [],
   "source": [
    "cifar_train_norm = cifar_train.astype('float32') / 255\n",
    "cifar_test_norm = cifar_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8GIUxRcq_Yc"
   },
   "outputs": [],
   "source": [
    "cifar_train_labels = to_categorical(cifar_train_labels_sparse)\n",
    "cifar_test_labels = to_categorical(cifar_test_labels_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "hrex5OcCEJIC",
    "outputId": "ef603b66-0dfc-4c5b-cd35-fec82fa80abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cifar_test_labels[1])\n",
    "cifar_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lloZq0DCeO7P"
   },
   "outputs": [],
   "source": [
    "# for TPU\n",
    "\n",
    "def train_input_fn(batch_size=1024):\n",
    "  # Convert the inputs to a Dataset.\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((cifar_train_norm, cifar_train_labels))\n",
    "\n",
    "  # Shuffle, repeat, and batch the examples.\n",
    "  dataset = dataset.cache()\n",
    "  dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n",
    "  dataset = dataset.repeat()\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "  return dataset\n",
    "\n",
    "cifar_train_tpu = train_input_fn(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BiAFGIhNDgoq"
   },
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6L8zGxnDi0O"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiq5LmixDx0t"
   },
   "outputs": [],
   "source": [
    "(fmnist_train, fmnist_train_labels_sparse), (fmnist_test, fmnist_test_labels_sparse) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_a_HVfMD1wQ"
   },
   "outputs": [],
   "source": [
    "fmnist_train_norm = fmnist_train.astype('float32') / 255\n",
    "fmnist_test_norm = fmnist_test.astype('float32') / 255\n",
    "\n",
    "fmnist_train_labels = to_categorical(fmnist_train_labels_sparse)\n",
    "fmnist_test_labels = to_categorical(fmnist_test_labels_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsDq3GrzMQoO"
   },
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUAV-O3fMTXn"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2ZMAwp_WMXIx",
    "outputId": "70b442d1-ac68-4a7f-b077-dcd121cfc478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(mnist_train, mnist_train_labels_sparse), (mnist_test, mnist_test_labels_sparse) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfuXVVS6McDQ"
   },
   "outputs": [],
   "source": [
    "mnist_train_norm = mnist_train.astype('float32') / 255\n",
    "mnist_test_norm = mnist_test.astype('float32') / 255\n",
    "\n",
    "mnist_train_labels = to_categorical(mnist_train_labels_sparse)\n",
    "mnist_test_labels = to_categorical(mnist_test_labels_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HB4tl71n5NRo"
   },
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGrAd2LbriPX"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, UpSampling2D, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15Pea-6fNESS"
   },
   "source": [
    "## Simple CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkduO2Qwrxpq"
   },
   "outputs": [],
   "source": [
    "def modelSimpleCNN(num_classes=10, input_shape=(28, 28)):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(filters=28, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(MaxPool2D())\n",
    "  model.add(Conv2D(filters=56, kernel_size=(3, 3), activation='relu'))\n",
    "  model.add(MaxPool2D())\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(num_classes, activation='softmax'))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHCRfFwuNtq_"
   },
   "outputs": [],
   "source": [
    "def modelSimpleCNN2():\n",
    "  model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation=tf.nn.relu),\n",
    "    Dense(10, activation=tf.nn.softmax)\n",
    "  ])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUB7NCtSOhKr"
   },
   "outputs": [],
   "source": [
    "# SEE: https://keras.io/examples/cifar10_cnn/\n",
    "def modelDeeperCNN(num_classes=10, input_shape=(32, 32, 3)):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvlNsw21Opul"
   },
   "source": [
    "## UNet\n",
    "thx to [zhixuhao](https://github.com/zhixuhao/unet/blob/master/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PGh_VTBOtwp"
   },
   "outputs": [],
   "source": [
    "def modelUnet(input_size = (256,256,1)):\n",
    "  inputs = Input(input_size)\n",
    "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "  drop4 = Dropout(0.5)(conv4)\n",
    "  pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "  drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "  up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "  merge6 = concatenate([drop4,up6], axis = 3)\n",
    "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "  up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "  merge7 = concatenate([conv3,up7], axis = 3)\n",
    "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "  up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "  merge8 = concatenate([conv2,up8], axis = 3)\n",
    "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "  up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "  merge9 = concatenate([conv1,up9], axis = 3)\n",
    "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "  conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "  conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "  model = Model(input = inputs, output = conv10)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFUizZo6MkvV"
   },
   "source": [
    "## CapsNet\n",
    "\n",
    "thx to [keras docs](https://keras.io/examples/cifar10_cnn_capsule/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54M7og6ZM48b"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWGSZ4awMvcD"
   },
   "outputs": [],
   "source": [
    "# the squashing function.\n",
    "# we use 0.5 in stead of 1 in hinton's paper.\n",
    "# if 1, the norm of vector will be zoomed out.\n",
    "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
    "# and be zoomed out while original norm is greater than 0.5.\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "# define our own softmax function instead of K.softmax\n",
    "# because K.softmax can not specify axis.\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IP6Qc9laM2uh"
   },
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    \"\"\"A Capsule Implement with Pure Keras\n",
    "    There are two vesions of Capsule.\n",
    "    One is like dense layer (for the fixed-shape input),\n",
    "    and the other is like timedistributed dense (for various length input).\n",
    "\n",
    "    The input shape of Capsule must be (batch_size,\n",
    "                                        input_num_capsule,\n",
    "                                        input_dim_capsule\n",
    "                                       )\n",
    "    and the output shape is (batch_size,\n",
    "                             num_capsule,\n",
    "                             dim_capsule\n",
    "                            )\n",
    "\n",
    "    Capsule Implement is from https://github.com/bojone/Capsule/\n",
    "    Capsule Paper: https://arxiv.org/abs/1710.09829\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_capsule,\n",
    "                 dim_capsule,\n",
    "                 routings=3,\n",
    "                 share_weights=True,\n",
    "                 activation='squash',\n",
    "                 **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'squash':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(1, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(input_num_capsule, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
    "        but replace b = b + <u,v> with b = <u,v>.\n",
    "\n",
    "        This change can improve the feature representation of Capsule.\n",
    "\n",
    "        However, you can replace\n",
    "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
    "        with\n",
    "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
    "        to realize a standard routing.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.share_weights:\n",
    "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
    "        else:\n",
    "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(inputs)[0]\n",
    "        input_num_capsule = K.shape(inputs)[1]\n",
    "        hat_inputs = K.reshape(hat_inputs,\n",
    "                               (batch_size, input_num_capsule,\n",
    "                                self.num_capsule, self.dim_capsule))\n",
    "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
    "\n",
    "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
    "        for i in range(self.routings):\n",
    "            c = softmax(b, 1)\n",
    "            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(o, hat_inputs, [2, 3])\n",
    "                if K.backend() == 'theano':\n",
    "                    o = K.sum(o, axis=1)\n",
    "\n",
    "        return o\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avqFYmxFNDIL"
   },
   "outputs": [],
   "source": [
    "def modelCapsNet():\n",
    "  # A common Conv2D model\n",
    "  input_image = Input(shape=(None, None, 3))\n",
    "  x = Conv2D(64, (3, 3), activation='relu')(input_image)\n",
    "  x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "  x = AveragePooling2D((2, 2))(x)\n",
    "  x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "  x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "\n",
    "\n",
    "  \"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "  then connect a Capsule layer.\n",
    "\n",
    "  the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "\n",
    "  the length of Capsule is the proba,\n",
    "  so the problem becomes a 10 two-classification problem.\n",
    "  \"\"\"\n",
    "\n",
    "  x = Reshape((-1, 128))(x)\n",
    "  capsule = Capsule(10, 16, 3, True)(x)\n",
    "  output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n",
    "  model = Model(inputs=input_image, outputs=output)\n",
    "\n",
    "  # we use a margin loss\n",
    "  # model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "  # model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2KXDRjJ7Non0"
   },
   "outputs": [],
   "source": [
    "# testCapsNet = modelCapsNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNQ9n_NFNi6F"
   },
   "source": [
    "## Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "30lPhgq2tSGh",
    "outputId": "68de9f2b-07ab-4809-86eb-cdb6bb0ec0cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# model = modelDeeperCNN()\n",
    "model = modelSimpleCNN(input_shape=(32, 32, 3))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "loss = 'categorical_crossentropy' # 'sparse_categorical_crossentropy'\n",
    "# opt = 'adam'\n",
    "# opt = RMSprop(lr=learning_rate, decay=1e-6)\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "#Â print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POFObjd0td4x"
   },
   "outputs": [],
   "source": [
    "# history = model.fit(x_train, y_train, batch_size=50, epochs=15, verbose=1, validation_data=(x_test, y_test))\n",
    "# model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "start = timer()\n",
    "model.fit(cifar_train_norm, cifar_train_labels, batch_size=batch_size, epochs=epochs, validation_data=(cifar_test_norm, cifar_test_labels), shuffle=True)\n",
    "\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "usMMcfFofKlW",
    "outputId": "7ff28fdf-5010-4d25-b437-67fbb731af75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.109.223.170:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 7150109408391237144)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 18182718921799644587)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5459426430569311791)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 14708536734863122741)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6936600185868770811)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 17711083087186384688)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2951895544517852767)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 18356347227509062402)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 873161690309816032)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14839761406660498960)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 2518240021943811026)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
     ]
    }
   ],
   "source": [
    "# TPU training\n",
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1905
    },
    "colab_type": "code",
    "id": "2HZx6TUXf74R",
    "outputId": "370c0400-cb6c-4ba5-9618-0466ad815b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(1024,), dtype=tf.int32, name=None), TensorSpec(shape=(1024, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(1024, 10), dtype=tf.float32, name=None)]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_input\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 2.2270395755767822 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "60/60 [==============================] - 8s 137ms/step - loss: 2.2492 - acc: 0.2033\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 2.0450 - acc: 0.3095\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.8690 - acc: 0.3536\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.7530 - acc: 0.3936\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 1.6577 - acc: 0.4252\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.5809 - acc: 0.4511\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.5187 - acc: 0.4721\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.4689 - acc: 0.4902\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.4289 - acc: 0.5051\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.3955 - acc: 0.5175\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.3665 - acc: 0.5281\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.3424 - acc: 0.5374\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.3199 - acc: 0.5454\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.3003 - acc: 0.5534\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.2833 - acc: 0.5597\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.2669 - acc: 0.5663\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.2519 - acc: 0.5717\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.2382 - acc: 0.5765\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 1.2252 - acc: 0.5811\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.2128 - acc: 0.5861\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.2024 - acc: 0.5895\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.1918 - acc: 0.5935\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.1818 - acc: 0.5973\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.1726 - acc: 0.6005\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.1638 - acc: 0.6036\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 1.1551 - acc: 0.6066\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.1482 - acc: 0.6091\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.1404 - acc: 0.6115\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.1329 - acc: 0.6145\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 1.1268 - acc: 0.6162\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.1197 - acc: 0.6185\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.1133 - acc: 0.6209\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 1.1082 - acc: 0.6227\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 1.1018 - acc: 0.6249\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 4s 63ms/step - loss: 1.0964 - acc: 0.6269\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 1.0912 - acc: 0.6284\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.0854 - acc: 0.6304\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.0807 - acc: 0.6323\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.0765 - acc: 0.6336\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.0713 - acc: 0.6351\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.0670 - acc: 0.6367\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.0626 - acc: 0.6381\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.0577 - acc: 0.6398\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 4s 67ms/step - loss: 1.0538 - acc: 0.6411\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.0503 - acc: 0.6423\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.0460 - acc: 0.6440\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.0426 - acc: 0.6450\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 1.0384 - acc: 0.6462\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 4s 65ms/step - loss: 1.0345 - acc: 0.6475\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 4s 66ms/step - loss: 1.0312 - acc: 0.6485\n",
      "0:03:29.354715\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 1024\n",
    "\n",
    "start = timer()\n",
    "# model.fit(cifar_train_norm, cifar_train_labels, batch_size=batch_size, epochs=epochs, validation_data=(cifar_test_norm, cifar_test_labels), shuffle=True)\n",
    "history = tpu_model.fit(\n",
    "    train_input_fn,\n",
    "    # batch_size=batch_size,\n",
    "    steps_per_epoch = 60,\n",
    "    epochs=epochs,\n",
    "    # validation_data=(cifar_test_norm, cifar_test_labels)\n",
    "    # validation_split=0.2\n",
    ")\n",
    "\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3zh6_YljSBR"
   },
   "outputs": [],
   "source": [
    "tpu_model.evaluate(cifar_test_norm, cifar_test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sVWtDWnkCCF"
   },
   "outputs": [],
   "source": [
    "tpu_model.evaluate(cifar_test_norm, cifar_test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBV8UkTHD-2v"
   },
   "outputs": [],
   "source": [
    "# reset altogether\n",
    "# tf.reset_default_graph()\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# reset weights\n",
    "\n",
    "trained_weights = tpu_model.get_weights()\n",
    "empty_weights = np.zeros(trained_weights[0].shape)\n",
    "\n",
    "tpu_model.set_weights([empty_weights])\n",
    "\n",
    "tpu_model.evaluate(cifar_test_norm, cifar_test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uPZzCXJnInlE"
   },
   "outputs": [],
   "source": [
    "tpu_model.set_weights(trained_weights)\n",
    "tpu_model.evaluate(cifar_test_norm, cifar_test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "be_16urbKdx-"
   },
   "outputs": [],
   "source": [
    "tpu_model.save_weights('deeper_cnn_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K72cikan8LjA"
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqKScBsxAeoF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrHvA3GG8NfI"
   },
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "training_acc = history.history['acc']\n",
    "\n",
    "# test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "id": "ixYknyTv8U7_",
    "outputId": "f3bb3ea7-f64b-4c45-81bc-0aac8aba9ca5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAycAAAIlCAYAAADPFocWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize loss history\n",
    "#plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, training_acc, 'b--')\n",
    "#plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "makL4koWOu0M"
   },
   "source": [
    "# Cross-validation\n",
    "\n",
    "https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "OVFVx2zEOxPx",
    "outputId": "992fc74d-cb27-4430-aa49-e6d70a52706b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.3'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJ87NdnEQ6Af"
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "folds = 5\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "XsXhElBqRGt8",
    "outputId": "3f1f6c39-e33a-4b98-a00f-ec4b4760adfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.108.169.146:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4384950252305678146)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9165701345706545634)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 17010715377276071108)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12750756930083673898)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12754339385921374221)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7627658415168390770)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1675441817483727495)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1589006882316867226)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 12091166294177898435)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 1723516223153676942)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 14460961700103411615)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 40\n",
    "batch_size = 1024\n",
    "\n",
    "learning_rate = 0.0001\n",
    "loss = 'categorical_crossentropy'\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "model = modelDeeperCNN()\n",
    "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdpIT76m9Bjl"
   },
   "outputs": [],
   "source": [
    "initial_weights = tpu_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33874
    },
    "colab_type": "code",
    "id": "y078QOBTSu6S",
    "outputId": "76ce8123-a463-4cbe-9223-654fc364936f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "Cross-validation fold 1\n",
      "Epoch 1/200\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(1024,), dtype=tf.int32, name=None), TensorSpec(shape=(1024, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(1024, 10), dtype=tf.float32, name=None)]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_input\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 4.627600431442261 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 2.2028 - acc: 0.1792\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.8846 - acc: 0.3142\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.7068 - acc: 0.3825\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.6105 - acc: 0.4160\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.5456 - acc: 0.4407\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4929 - acc: 0.4597\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4494 - acc: 0.4770\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4089 - acc: 0.4919\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.3748 - acc: 0.5043\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 1.3415 - acc: 0.5175\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.3140 - acc: 0.5292\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2881 - acc: 0.5397\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2622 - acc: 0.5495\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2377 - acc: 0.5592\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.2176 - acc: 0.5676\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1945 - acc: 0.5757\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.1729 - acc: 0.5841\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.1535 - acc: 0.5910\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1349 - acc: 0.5987\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1155 - acc: 0.6060\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0978 - acc: 0.6120\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0776 - acc: 0.6202\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0617 - acc: 0.6258\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0442 - acc: 0.6331\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0271 - acc: 0.6385\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0122 - acc: 0.6448\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9954 - acc: 0.6504\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9793 - acc: 0.6567\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9670 - acc: 0.6611\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9519 - acc: 0.6666\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9397 - acc: 0.6711\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.9227 - acc: 0.6769\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.9090 - acc: 0.6822\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.8953 - acc: 0.6873\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.8829 - acc: 0.6921\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8700 - acc: 0.6969\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8569 - acc: 0.7019\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8444 - acc: 0.7050\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8322 - acc: 0.7104\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8185 - acc: 0.7155\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8092 - acc: 0.7183\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7969 - acc: 0.7220\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7834 - acc: 0.7269\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.7742 - acc: 0.7309\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7627 - acc: 0.7342\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7512 - acc: 0.7380\n",
      "Epoch 47/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7416 - acc: 0.7417\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.7295 - acc: 0.7454\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7205 - acc: 0.7490\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7114 - acc: 0.7525\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7002 - acc: 0.7565\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6916 - acc: 0.7594\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6829 - acc: 0.7628\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6738 - acc: 0.7646\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.6637 - acc: 0.7689\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.6557 - acc: 0.7720\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6448 - acc: 0.7751\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6364 - acc: 0.7783\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6269 - acc: 0.7810\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6199 - acc: 0.7832\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6107 - acc: 0.7869\n",
      "Epoch 62/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.6043 - acc: 0.7882\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5961 - acc: 0.7922\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5870 - acc: 0.7945\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5783 - acc: 0.7983\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5708 - acc: 0.8003\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5644 - acc: 0.8025\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5564 - acc: 0.8051\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.5501 - acc: 0.8077\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5417 - acc: 0.8101\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.5339 - acc: 0.8134\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5269 - acc: 0.8151\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.5184 - acc: 0.8178\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5145 - acc: 0.8193\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5049 - acc: 0.8231\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4988 - acc: 0.8259\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.4921 - acc: 0.8268\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4851 - acc: 0.8297\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4787 - acc: 0.8313\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4735 - acc: 0.8332\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4658 - acc: 0.8362\n",
      "Epoch 82/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4605 - acc: 0.8373\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4540 - acc: 0.8394\n",
      "Epoch 84/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4473 - acc: 0.8429\n",
      "Epoch 85/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4425 - acc: 0.8442\n",
      "Epoch 86/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.4357 - acc: 0.8466\n",
      "Epoch 87/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.4296 - acc: 0.8482\n",
      "Epoch 88/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.4262 - acc: 0.8499\n",
      "Epoch 89/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.4175 - acc: 0.8526\n",
      "Epoch 90/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4120 - acc: 0.8546\n",
      "Epoch 91/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4085 - acc: 0.8557\n",
      "Epoch 92/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4029 - acc: 0.8578\n",
      "Epoch 93/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3972 - acc: 0.8595\n",
      "Epoch 94/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3924 - acc: 0.8606\n",
      "Epoch 95/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3872 - acc: 0.8636\n",
      "Epoch 96/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3819 - acc: 0.8648\n",
      "Epoch 97/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3758 - acc: 0.8674\n",
      "Epoch 98/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3711 - acc: 0.8688\n",
      "Epoch 99/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3658 - acc: 0.8706\n",
      "Epoch 100/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3621 - acc: 0.8720\n",
      "Epoch 101/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3560 - acc: 0.8742\n",
      "Epoch 102/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3521 - acc: 0.8747\n",
      "Epoch 103/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3476 - acc: 0.8770\n",
      "Epoch 104/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3418 - acc: 0.8793\n",
      "Epoch 105/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3380 - acc: 0.8800\n",
      "Epoch 106/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3336 - acc: 0.8817\n",
      "Epoch 107/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3305 - acc: 0.8832\n",
      "Epoch 108/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3258 - acc: 0.8845\n",
      "Epoch 109/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3231 - acc: 0.8852\n",
      "Epoch 110/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3182 - acc: 0.8872\n",
      "Epoch 111/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3144 - acc: 0.8881\n",
      "Epoch 112/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3113 - acc: 0.8894\n",
      "Epoch 113/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3072 - acc: 0.8906\n",
      "Epoch 114/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3045 - acc: 0.8918\n",
      "Epoch 115/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2985 - acc: 0.8941\n",
      "Epoch 116/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2951 - acc: 0.8953\n",
      "Epoch 117/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2912 - acc: 0.8964\n",
      "Epoch 118/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2873 - acc: 0.8984\n",
      "Epoch 119/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2868 - acc: 0.8975\n",
      "Epoch 120/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2806 - acc: 0.9006\n",
      "Epoch 121/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2777 - acc: 0.9014\n",
      "Epoch 122/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2755 - acc: 0.9023\n",
      "Epoch 123/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2724 - acc: 0.9033\n",
      "Epoch 124/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2692 - acc: 0.9040\n",
      "Epoch 125/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2660 - acc: 0.9052\n",
      "Epoch 126/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2633 - acc: 0.9067\n",
      "Epoch 127/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2579 - acc: 0.9083\n",
      "Epoch 128/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2584 - acc: 0.9087\n",
      "Epoch 129/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2527 - acc: 0.9099\n",
      "Epoch 130/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2516 - acc: 0.9113\n",
      "Epoch 131/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2464 - acc: 0.9127\n",
      "Epoch 132/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2439 - acc: 0.9134\n",
      "Epoch 133/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2413 - acc: 0.9142\n",
      "Epoch 134/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2398 - acc: 0.9146\n",
      "Epoch 135/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2383 - acc: 0.9151\n",
      "Epoch 136/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2346 - acc: 0.9168\n",
      "Epoch 137/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2324 - acc: 0.9176\n",
      "Epoch 138/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2289 - acc: 0.9188\n",
      "Epoch 139/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2279 - acc: 0.9190\n",
      "Epoch 140/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2262 - acc: 0.9199\n",
      "Epoch 141/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2233 - acc: 0.9205\n",
      "Epoch 142/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2210 - acc: 0.9219\n",
      "Epoch 143/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2182 - acc: 0.9224\n",
      "Epoch 144/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2156 - acc: 0.9235\n",
      "Epoch 145/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2169 - acc: 0.9226\n",
      "Epoch 146/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2121 - acc: 0.9250\n",
      "Epoch 147/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2091 - acc: 0.9259\n",
      "Epoch 148/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2084 - acc: 0.9265\n",
      "Epoch 149/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2055 - acc: 0.9274\n",
      "Epoch 150/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2044 - acc: 0.9277\n",
      "Epoch 151/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2034 - acc: 0.9278\n",
      "Epoch 152/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1998 - acc: 0.9294\n",
      "Epoch 153/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1992 - acc: 0.9296\n",
      "Epoch 154/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1968 - acc: 0.9302\n",
      "Epoch 155/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1930 - acc: 0.9317\n",
      "Epoch 156/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1923 - acc: 0.9320\n",
      "Epoch 157/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1903 - acc: 0.9328\n",
      "Epoch 158/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1893 - acc: 0.9329\n",
      "Epoch 159/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1865 - acc: 0.9339\n",
      "Epoch 160/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1849 - acc: 0.9350\n",
      "Epoch 161/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1847 - acc: 0.9348\n",
      "Epoch 162/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1821 - acc: 0.9356\n",
      "Epoch 163/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1816 - acc: 0.9359\n",
      "Epoch 164/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1786 - acc: 0.9370\n",
      "Epoch 165/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1782 - acc: 0.9373\n",
      "Epoch 166/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1770 - acc: 0.9372\n",
      "Epoch 167/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1744 - acc: 0.9384\n",
      "Epoch 168/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1737 - acc: 0.9389\n",
      "Epoch 169/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1718 - acc: 0.9396\n",
      "Epoch 170/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1698 - acc: 0.9403\n",
      "Epoch 171/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1691 - acc: 0.9405\n",
      "Epoch 172/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.1683 - acc: 0.9405\n",
      "Epoch 173/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1675 - acc: 0.9411\n",
      "Epoch 174/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1664 - acc: 0.9415\n",
      "Epoch 175/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1640 - acc: 0.9420\n",
      "Epoch 176/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1619 - acc: 0.9427\n",
      "Epoch 177/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1600 - acc: 0.9434\n",
      "Epoch 178/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1603 - acc: 0.9433\n",
      "Epoch 179/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1584 - acc: 0.9444\n",
      "Epoch 180/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1568 - acc: 0.9450\n",
      "Epoch 181/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1563 - acc: 0.9450\n",
      "Epoch 182/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1559 - acc: 0.9454\n",
      "Epoch 183/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1522 - acc: 0.9460\n",
      "Epoch 184/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1517 - acc: 0.9466\n",
      "Epoch 185/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1526 - acc: 0.9461\n",
      "Epoch 186/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1500 - acc: 0.9473\n",
      "Epoch 187/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1499 - acc: 0.9469\n",
      "Epoch 188/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1493 - acc: 0.9472\n",
      "Epoch 189/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1459 - acc: 0.9488\n",
      "Epoch 190/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1464 - acc: 0.9485\n",
      "Epoch 191/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1445 - acc: 0.9491\n",
      "Epoch 192/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1443 - acc: 0.9488\n",
      "Epoch 193/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1421 - acc: 0.9502\n",
      "Epoch 194/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1397 - acc: 0.9510\n",
      "Epoch 195/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1407 - acc: 0.9508\n",
      "Epoch 196/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1401 - acc: 0.9505\n",
      "Epoch 197/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1376 - acc: 0.9517\n",
      "Epoch 198/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1386 - acc: 0.9515\n",
      "Epoch 199/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1351 - acc: 0.9526\n",
      "Epoch 200/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1373 - acc: 0.9517\n",
      "---------------------\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 32, 32, 3), dtype=tf.float32, name='conv2d_input_10'), TensorSpec(shape=(128, 10), dtype=tf.float32, name='activation_5_target_30')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 1.389955759048462 secs\n",
      " 9216/10000 [==========================>...] - ETA: 0s - loss: 0.8056 - acc: 0.7875INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(98,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(98, 32, 32, 3), dtype=tf.float32, name='conv2d_input_10'), TensorSpec(shape=(98, 10), dtype=tf.float32, name='activation_5_target_30')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 1.5821595191955566 secs\n",
      "10000/10000 [==============================] - 5s 489us/sample - loss: 0.8038 - acc: 0.7883\n",
      "============================\n",
      "Cross-validation fold 2\n",
      "Epoch 1/200\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 2.2808 - acc: 0.1308\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 2.1205 - acc: 0.2194\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.9405 - acc: 0.2933\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.7932 - acc: 0.3498\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.7050 - acc: 0.3796\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.6477 - acc: 0.4013\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.6108 - acc: 0.4146\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.5790 - acc: 0.4275\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.5497 - acc: 0.4390\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.5261 - acc: 0.4485\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.5004 - acc: 0.4586\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4772 - acc: 0.4675\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.4555 - acc: 0.4766\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.4326 - acc: 0.4854\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4108 - acc: 0.4930\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.3907 - acc: 0.5011\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.3678 - acc: 0.5102\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3485 - acc: 0.5174\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.3277 - acc: 0.5250\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3082 - acc: 0.5322\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2887 - acc: 0.5402\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2685 - acc: 0.5476\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2526 - acc: 0.5545\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2354 - acc: 0.5611\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2189 - acc: 0.5677\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2000 - acc: 0.5733\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1846 - acc: 0.5790\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1706 - acc: 0.5853\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1552 - acc: 0.5910\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1381 - acc: 0.5977\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1249 - acc: 0.6024\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1090 - acc: 0.6085\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0989 - acc: 0.6129\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0821 - acc: 0.6176\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0686 - acc: 0.6228\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0543 - acc: 0.6291\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0408 - acc: 0.6337\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0282 - acc: 0.6381\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0144 - acc: 0.6434\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0011 - acc: 0.6486\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9898 - acc: 0.6531\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9749 - acc: 0.6587\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.9644 - acc: 0.6617\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9514 - acc: 0.6670\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.9380 - acc: 0.6711\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9285 - acc: 0.6758\n",
      "Epoch 47/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9150 - acc: 0.6799\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9040 - acc: 0.6838\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8929 - acc: 0.6885\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8803 - acc: 0.6923\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8694 - acc: 0.6968\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.8560 - acc: 0.7009\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8452 - acc: 0.7047\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8352 - acc: 0.7073\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8248 - acc: 0.7118\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8155 - acc: 0.7155\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8034 - acc: 0.7200\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7908 - acc: 0.7241\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 0.7805 - acc: 0.7269\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.7711 - acc: 0.7318\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7617 - acc: 0.7343\n",
      "Epoch 62/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7515 - acc: 0.7383\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7411 - acc: 0.7414\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.7315 - acc: 0.7438\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.7223 - acc: 0.7472\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7117 - acc: 0.7518\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7028 - acc: 0.7550\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6934 - acc: 0.7579\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6834 - acc: 0.7616\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6746 - acc: 0.7638\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6654 - acc: 0.7683\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6555 - acc: 0.7700\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6454 - acc: 0.7740\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6387 - acc: 0.7763\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6303 - acc: 0.7794\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6195 - acc: 0.7833\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6121 - acc: 0.7852\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6010 - acc: 0.7894\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5947 - acc: 0.7914\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5858 - acc: 0.7940\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5767 - acc: 0.7969\n",
      "Epoch 82/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5690 - acc: 0.8002\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5615 - acc: 0.8025\n",
      "Epoch 84/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5542 - acc: 0.8059\n",
      "Epoch 85/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5451 - acc: 0.8081\n",
      "Epoch 86/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5360 - acc: 0.8117\n",
      "Epoch 87/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.5270 - acc: 0.8148\n",
      "Epoch 88/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5208 - acc: 0.8165\n",
      "Epoch 89/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5140 - acc: 0.8187\n",
      "Epoch 90/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5070 - acc: 0.8206\n",
      "Epoch 91/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4990 - acc: 0.8240\n",
      "Epoch 92/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4917 - acc: 0.8268\n",
      "Epoch 93/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4869 - acc: 0.8278\n",
      "Epoch 94/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4800 - acc: 0.8306\n",
      "Epoch 95/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4710 - acc: 0.8334\n",
      "Epoch 96/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4657 - acc: 0.8360\n",
      "Epoch 97/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4572 - acc: 0.8380\n",
      "Epoch 98/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4507 - acc: 0.8414\n",
      "Epoch 99/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4441 - acc: 0.8427\n",
      "Epoch 100/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4397 - acc: 0.8447\n",
      "Epoch 101/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4328 - acc: 0.8465\n",
      "Epoch 102/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4225 - acc: 0.8508\n",
      "Epoch 103/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4172 - acc: 0.8520\n",
      "Epoch 104/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4137 - acc: 0.8534\n",
      "Epoch 105/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4078 - acc: 0.8558\n",
      "Epoch 106/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4019 - acc: 0.8576\n",
      "Epoch 107/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3943 - acc: 0.8600\n",
      "Epoch 108/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3897 - acc: 0.8624\n",
      "Epoch 109/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3826 - acc: 0.8644\n",
      "Epoch 110/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.3778 - acc: 0.8658\n",
      "Epoch 111/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.3733 - acc: 0.8677\n",
      "Epoch 112/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.3694 - acc: 0.8690\n",
      "Epoch 113/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3617 - acc: 0.8719\n",
      "Epoch 114/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3591 - acc: 0.8724\n",
      "Epoch 115/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3529 - acc: 0.8748\n",
      "Epoch 116/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3476 - acc: 0.8765\n",
      "Epoch 117/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.3440 - acc: 0.8779\n",
      "Epoch 118/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.3378 - acc: 0.8801\n",
      "Epoch 119/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3336 - acc: 0.8817\n",
      "Epoch 120/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3304 - acc: 0.8827\n",
      "Epoch 121/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3234 - acc: 0.8850\n",
      "Epoch 122/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3197 - acc: 0.8860\n",
      "Epoch 123/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3175 - acc: 0.8876\n",
      "Epoch 124/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3116 - acc: 0.8892\n",
      "Epoch 125/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3074 - acc: 0.8910\n",
      "Epoch 126/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3030 - acc: 0.8927\n",
      "Epoch 127/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2996 - acc: 0.8938\n",
      "Epoch 128/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2974 - acc: 0.8943\n",
      "Epoch 129/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2936 - acc: 0.8957\n",
      "Epoch 130/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2903 - acc: 0.8966\n",
      "Epoch 131/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2845 - acc: 0.8989\n",
      "Epoch 132/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2818 - acc: 0.8999\n",
      "Epoch 133/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2788 - acc: 0.9007\n",
      "Epoch 134/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2756 - acc: 0.9017\n",
      "Epoch 135/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2720 - acc: 0.9033\n",
      "Epoch 136/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2696 - acc: 0.9045\n",
      "Epoch 137/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2646 - acc: 0.9058\n",
      "Epoch 138/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2633 - acc: 0.9063\n",
      "Epoch 139/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2573 - acc: 0.9082\n",
      "Epoch 140/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2580 - acc: 0.9078\n",
      "Epoch 141/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.2528 - acc: 0.9104\n",
      "Epoch 142/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2506 - acc: 0.9106\n",
      "Epoch 143/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2493 - acc: 0.9108\n",
      "Epoch 144/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2467 - acc: 0.9124\n",
      "Epoch 145/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2418 - acc: 0.9143\n",
      "Epoch 146/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2408 - acc: 0.9143\n",
      "Epoch 147/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2382 - acc: 0.9156\n",
      "Epoch 148/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2348 - acc: 0.9168\n",
      "Epoch 149/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2334 - acc: 0.9175\n",
      "Epoch 150/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2305 - acc: 0.9180\n",
      "Epoch 151/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2277 - acc: 0.9187\n",
      "Epoch 152/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2255 - acc: 0.9195\n",
      "Epoch 153/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2216 - acc: 0.9216\n",
      "Epoch 154/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2223 - acc: 0.9211\n",
      "Epoch 155/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2193 - acc: 0.9226\n",
      "Epoch 156/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2171 - acc: 0.9231\n",
      "Epoch 157/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2142 - acc: 0.9240\n",
      "Epoch 158/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2124 - acc: 0.9247\n",
      "Epoch 159/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2098 - acc: 0.9257\n",
      "Epoch 160/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2084 - acc: 0.9255\n",
      "Epoch 161/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2071 - acc: 0.9269\n",
      "Epoch 162/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2037 - acc: 0.9277\n",
      "Epoch 163/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2008 - acc: 0.9286\n",
      "Epoch 164/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1993 - acc: 0.9295\n",
      "Epoch 165/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1970 - acc: 0.9303\n",
      "Epoch 166/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1954 - acc: 0.9312\n",
      "Epoch 167/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1939 - acc: 0.9312\n",
      "Epoch 168/200\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.1926 - acc: 0.9317\n",
      "Epoch 169/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1922 - acc: 0.9323\n",
      "Epoch 170/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1893 - acc: 0.9329\n",
      "Epoch 171/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1872 - acc: 0.9341\n",
      "Epoch 172/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1875 - acc: 0.9335\n",
      "Epoch 173/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1839 - acc: 0.9345\n",
      "Epoch 174/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1826 - acc: 0.9352\n",
      "Epoch 175/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1810 - acc: 0.9360\n",
      "Epoch 176/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1806 - acc: 0.9366\n",
      "Epoch 177/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1796 - acc: 0.9368\n",
      "Epoch 178/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1753 - acc: 0.9382\n",
      "Epoch 179/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1768 - acc: 0.9375\n",
      "Epoch 180/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1738 - acc: 0.9386\n",
      "Epoch 181/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1728 - acc: 0.9388\n",
      "Epoch 182/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1706 - acc: 0.9398\n",
      "Epoch 183/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1695 - acc: 0.9399\n",
      "Epoch 184/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1682 - acc: 0.9407\n",
      "Epoch 185/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1681 - acc: 0.9408\n",
      "Epoch 186/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1660 - acc: 0.9411\n",
      "Epoch 187/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1650 - acc: 0.9421\n",
      "Epoch 188/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1611 - acc: 0.9432\n",
      "Epoch 189/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1632 - acc: 0.9423\n",
      "Epoch 190/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1608 - acc: 0.9438\n",
      "Epoch 191/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1581 - acc: 0.9441\n",
      "Epoch 192/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1574 - acc: 0.9449\n",
      "Epoch 193/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1582 - acc: 0.9442\n",
      "Epoch 194/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1541 - acc: 0.9455\n",
      "Epoch 195/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1546 - acc: 0.9454\n",
      "Epoch 196/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1534 - acc: 0.9459\n",
      "Epoch 197/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1520 - acc: 0.9464\n",
      "Epoch 198/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1511 - acc: 0.9466\n",
      "Epoch 199/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1484 - acc: 0.9477\n",
      "Epoch 200/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1486 - acc: 0.9474\n",
      "---------------------\n",
      "10000/10000 [==============================] - 1s 106us/sample - loss: 0.0079 - acc: 0.9998\n",
      "============================\n",
      "Cross-validation fold 3\n",
      "Epoch 1/200\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 2.2830 - acc: 0.1290\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 2.1303 - acc: 0.2148\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.9631 - acc: 0.2827\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.8354 - acc: 0.3359\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.7374 - acc: 0.3706\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.6800 - acc: 0.3906\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.6359 - acc: 0.4060\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.6000 - acc: 0.4207\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.5659 - acc: 0.4337\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.5369 - acc: 0.4437\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.5097 - acc: 0.4544\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4848 - acc: 0.4648\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4614 - acc: 0.4729\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4386 - acc: 0.4824\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.4172 - acc: 0.4906\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.3947 - acc: 0.4995\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3740 - acc: 0.5061\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3526 - acc: 0.5161\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.3297 - acc: 0.5240\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.3082 - acc: 0.5319\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.2886 - acc: 0.5392\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2664 - acc: 0.5474\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2480 - acc: 0.5546\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2291 - acc: 0.5628\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2112 - acc: 0.5687\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1938 - acc: 0.5750\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.1767 - acc: 0.5816\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1612 - acc: 0.5885\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1443 - acc: 0.5951\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1286 - acc: 0.6000\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1128 - acc: 0.6069\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0983 - acc: 0.6117\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0835 - acc: 0.6176\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0709 - acc: 0.6235\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0541 - acc: 0.6284\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0414 - acc: 0.6334\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0269 - acc: 0.6393\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0117 - acc: 0.6451\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.9970 - acc: 0.6502\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9834 - acc: 0.6541\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9699 - acc: 0.6595\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9558 - acc: 0.6644\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9411 - acc: 0.6703\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9276 - acc: 0.6752\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9138 - acc: 0.6801\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8993 - acc: 0.6851\n",
      "Epoch 47/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8867 - acc: 0.6896\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8723 - acc: 0.6947\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8572 - acc: 0.7005\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8447 - acc: 0.7045\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8312 - acc: 0.7098\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.8183 - acc: 0.7138\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8056 - acc: 0.7176\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7917 - acc: 0.7232\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7786 - acc: 0.7279\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7650 - acc: 0.7331\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7543 - acc: 0.7362\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7448 - acc: 0.7397\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7284 - acc: 0.7456\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.7192 - acc: 0.7475\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.7063 - acc: 0.7530\n",
      "Epoch 62/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6974 - acc: 0.7560\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6840 - acc: 0.7600\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6734 - acc: 0.7639\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6639 - acc: 0.7675\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6514 - acc: 0.7714\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6425 - acc: 0.7750\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6302 - acc: 0.7791\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.6226 - acc: 0.7817\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.6115 - acc: 0.7853\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6019 - acc: 0.7887\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5917 - acc: 0.7923\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5847 - acc: 0.7949\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5751 - acc: 0.7974\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5651 - acc: 0.8015\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5569 - acc: 0.8040\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.5466 - acc: 0.8072\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.5398 - acc: 0.8095\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5318 - acc: 0.8131\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.5228 - acc: 0.8151\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5138 - acc: 0.8188\n",
      "Epoch 82/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5066 - acc: 0.8212\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5000 - acc: 0.8234\n",
      "Epoch 84/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4918 - acc: 0.8264\n",
      "Epoch 85/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4831 - acc: 0.8297\n",
      "Epoch 86/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4775 - acc: 0.8306\n",
      "Epoch 87/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4704 - acc: 0.8334\n",
      "Epoch 88/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4603 - acc: 0.8374\n",
      "Epoch 89/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4545 - acc: 0.8397\n",
      "Epoch 90/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4489 - acc: 0.8411\n",
      "Epoch 91/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4391 - acc: 0.8453\n",
      "Epoch 92/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4341 - acc: 0.8459\n",
      "Epoch 93/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4284 - acc: 0.8479\n",
      "Epoch 94/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4216 - acc: 0.8511\n",
      "Epoch 95/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4133 - acc: 0.8528\n",
      "Epoch 96/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4077 - acc: 0.8551\n",
      "Epoch 97/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4027 - acc: 0.8568\n",
      "Epoch 98/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3975 - acc: 0.8588\n",
      "Epoch 99/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3899 - acc: 0.8615\n",
      "Epoch 100/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3852 - acc: 0.8633\n",
      "Epoch 101/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3794 - acc: 0.8648\n",
      "Epoch 102/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3728 - acc: 0.8678\n",
      "Epoch 103/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3685 - acc: 0.8694\n",
      "Epoch 104/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3649 - acc: 0.8706\n",
      "Epoch 105/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3583 - acc: 0.8726\n",
      "Epoch 106/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3519 - acc: 0.8749\n",
      "Epoch 107/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3479 - acc: 0.8766\n",
      "Epoch 108/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3434 - acc: 0.8776\n",
      "Epoch 109/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3388 - acc: 0.8795\n",
      "Epoch 110/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3334 - acc: 0.8812\n",
      "Epoch 111/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3277 - acc: 0.8835\n",
      "Epoch 112/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3246 - acc: 0.8846\n",
      "Epoch 113/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.3191 - acc: 0.8869\n",
      "Epoch 114/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3160 - acc: 0.8872\n",
      "Epoch 115/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3092 - acc: 0.8898\n",
      "Epoch 116/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3058 - acc: 0.8915\n",
      "Epoch 117/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3023 - acc: 0.8925\n",
      "Epoch 118/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2974 - acc: 0.8941\n",
      "Epoch 119/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2943 - acc: 0.8951\n",
      "Epoch 120/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2906 - acc: 0.8966\n",
      "Epoch 121/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2861 - acc: 0.8985\n",
      "Epoch 122/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2831 - acc: 0.8992\n",
      "Epoch 123/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2784 - acc: 0.9009\n",
      "Epoch 124/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2751 - acc: 0.9023\n",
      "Epoch 125/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2727 - acc: 0.9027\n",
      "Epoch 126/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2681 - acc: 0.9050\n",
      "Epoch 127/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2636 - acc: 0.9063\n",
      "Epoch 128/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2621 - acc: 0.9069\n",
      "Epoch 129/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2582 - acc: 0.9086\n",
      "Epoch 130/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2547 - acc: 0.9100\n",
      "Epoch 131/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2525 - acc: 0.9097\n",
      "Epoch 132/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2498 - acc: 0.9115\n",
      "Epoch 133/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2467 - acc: 0.9126\n",
      "Epoch 134/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2447 - acc: 0.9133\n",
      "Epoch 135/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2395 - acc: 0.9147\n",
      "Epoch 136/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2359 - acc: 0.9163\n",
      "Epoch 137/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2342 - acc: 0.9165\n",
      "Epoch 138/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2322 - acc: 0.9175\n",
      "Epoch 139/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2283 - acc: 0.9196\n",
      "Epoch 140/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2259 - acc: 0.9200\n",
      "Epoch 141/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2235 - acc: 0.9204\n",
      "Epoch 142/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2225 - acc: 0.9212\n",
      "Epoch 143/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2187 - acc: 0.9219\n",
      "Epoch 144/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2159 - acc: 0.9235\n",
      "Epoch 145/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2123 - acc: 0.9251\n",
      "Epoch 146/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2116 - acc: 0.9253\n",
      "Epoch 147/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2088 - acc: 0.9258\n",
      "Epoch 148/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2058 - acc: 0.9275\n",
      "Epoch 149/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2057 - acc: 0.9271\n",
      "Epoch 150/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2043 - acc: 0.9276\n",
      "Epoch 151/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2013 - acc: 0.9286\n",
      "Epoch 152/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1982 - acc: 0.9295\n",
      "Epoch 153/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1968 - acc: 0.9301\n",
      "Epoch 154/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1957 - acc: 0.9303\n",
      "Epoch 155/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1915 - acc: 0.9323\n",
      "Epoch 156/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1923 - acc: 0.9316\n",
      "Epoch 157/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1886 - acc: 0.9331\n",
      "Epoch 158/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1864 - acc: 0.9337\n",
      "Epoch 159/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1857 - acc: 0.9345\n",
      "Epoch 160/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1842 - acc: 0.9347\n",
      "Epoch 161/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1814 - acc: 0.9362\n",
      "Epoch 162/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1796 - acc: 0.9363\n",
      "Epoch 163/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1788 - acc: 0.9368\n",
      "Epoch 164/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1760 - acc: 0.9376\n",
      "Epoch 165/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1734 - acc: 0.9387\n",
      "Epoch 166/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1720 - acc: 0.9395\n",
      "Epoch 167/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1706 - acc: 0.9401\n",
      "Epoch 168/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1701 - acc: 0.9397\n",
      "Epoch 169/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1705 - acc: 0.9398\n",
      "Epoch 170/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1666 - acc: 0.9409\n",
      "Epoch 171/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1656 - acc: 0.9414\n",
      "Epoch 172/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1640 - acc: 0.9426\n",
      "Epoch 173/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1618 - acc: 0.9431\n",
      "Epoch 174/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1601 - acc: 0.9437\n",
      "Epoch 175/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1598 - acc: 0.9436\n",
      "Epoch 176/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1581 - acc: 0.9442\n",
      "Epoch 177/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1574 - acc: 0.9444\n",
      "Epoch 178/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1564 - acc: 0.9449\n",
      "Epoch 179/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1528 - acc: 0.9461\n",
      "Epoch 180/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1524 - acc: 0.9460\n",
      "Epoch 181/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1514 - acc: 0.9468\n",
      "Epoch 182/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1517 - acc: 0.9466\n",
      "Epoch 183/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1499 - acc: 0.9473\n",
      "Epoch 184/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1469 - acc: 0.9484\n",
      "Epoch 185/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1458 - acc: 0.9486\n",
      "Epoch 186/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1455 - acc: 0.9485\n",
      "Epoch 187/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1449 - acc: 0.9494\n",
      "Epoch 188/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1433 - acc: 0.9495\n",
      "Epoch 189/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.1419 - acc: 0.9501\n",
      "Epoch 190/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1406 - acc: 0.9511\n",
      "Epoch 191/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1385 - acc: 0.9513\n",
      "Epoch 192/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1391 - acc: 0.9510\n",
      "Epoch 193/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1384 - acc: 0.9511\n",
      "Epoch 194/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1370 - acc: 0.9518\n",
      "Epoch 195/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1366 - acc: 0.9519\n",
      "Epoch 196/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1353 - acc: 0.9525\n",
      "Epoch 197/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1336 - acc: 0.9531\n",
      "Epoch 198/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1353 - acc: 0.9526\n",
      "Epoch 199/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1317 - acc: 0.9534\n",
      "Epoch 200/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1310 - acc: 0.9541\n",
      "---------------------\n",
      "10000/10000 [==============================] - 1s 106us/sample - loss: 0.0062 - acc: 0.9999\n",
      "============================\n",
      "Cross-validation fold 4\n",
      "Epoch 1/200\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 2.2810 - acc: 0.1287\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 2.1175 - acc: 0.2193\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.9432 - acc: 0.2926\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.8052 - acc: 0.3470\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.7178 - acc: 0.3777\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.6616 - acc: 0.3960\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.6186 - acc: 0.4124\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.5831 - acc: 0.4241\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.5516 - acc: 0.4364\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.5215 - acc: 0.4493\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4961 - acc: 0.4582\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4686 - acc: 0.4697\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4413 - acc: 0.4789\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.4142 - acc: 0.4896\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.3882 - acc: 0.5003\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3636 - acc: 0.5095\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3399 - acc: 0.5186\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.3175 - acc: 0.5275\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2969 - acc: 0.5353\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2769 - acc: 0.5436\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2570 - acc: 0.5512\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2403 - acc: 0.5584\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2224 - acc: 0.5635\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2037 - acc: 0.5732\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.1868 - acc: 0.5788\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.1701 - acc: 0.5856\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1540 - acc: 0.5908\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1372 - acc: 0.5983\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1226 - acc: 0.6023\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1089 - acc: 0.6086\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0909 - acc: 0.6149\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0766 - acc: 0.6204\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0624 - acc: 0.6252\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0498 - acc: 0.6309\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0330 - acc: 0.6370\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0228 - acc: 0.6409\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0080 - acc: 0.6460\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9947 - acc: 0.6522\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.9806 - acc: 0.6552\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9682 - acc: 0.6608\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.9541 - acc: 0.6662\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9411 - acc: 0.6709\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9271 - acc: 0.6765\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9164 - acc: 0.6805\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9035 - acc: 0.6849\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8887 - acc: 0.6899\n",
      "Epoch 47/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8776 - acc: 0.6938\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8653 - acc: 0.6981\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8533 - acc: 0.7031\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8402 - acc: 0.7069\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.8268 - acc: 0.7120\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8175 - acc: 0.7152\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8058 - acc: 0.7190\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7934 - acc: 0.7231\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7862 - acc: 0.7262\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7733 - acc: 0.7314\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7622 - acc: 0.7341\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.7501 - acc: 0.7380\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.7397 - acc: 0.7412\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7303 - acc: 0.7451\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7212 - acc: 0.7483\n",
      "Epoch 62/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7076 - acc: 0.7532\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6997 - acc: 0.7559\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.6915 - acc: 0.7587\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6807 - acc: 0.7613\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6717 - acc: 0.7652\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6613 - acc: 0.7691\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6532 - acc: 0.7719\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6419 - acc: 0.7758\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.6328 - acc: 0.7776\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6247 - acc: 0.7812\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6153 - acc: 0.7844\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6055 - acc: 0.7882\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5968 - acc: 0.7907\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5897 - acc: 0.7934\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5803 - acc: 0.7964\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5718 - acc: 0.7989\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5629 - acc: 0.8021\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.5561 - acc: 0.8037\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5467 - acc: 0.8077\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5389 - acc: 0.8099\n",
      "Epoch 82/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5317 - acc: 0.8129\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.5234 - acc: 0.8155\n",
      "Epoch 84/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5149 - acc: 0.8185\n",
      "Epoch 85/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5048 - acc: 0.8220\n",
      "Epoch 86/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4993 - acc: 0.8237\n",
      "Epoch 87/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4917 - acc: 0.8268\n",
      "Epoch 88/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4835 - acc: 0.8287\n",
      "Epoch 89/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4776 - acc: 0.8314\n",
      "Epoch 90/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4690 - acc: 0.8340\n",
      "Epoch 91/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4631 - acc: 0.8367\n",
      "Epoch 92/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4540 - acc: 0.8392\n",
      "Epoch 93/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4466 - acc: 0.8423\n",
      "Epoch 94/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.4403 - acc: 0.8449\n",
      "Epoch 95/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4355 - acc: 0.8457\n",
      "Epoch 96/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4294 - acc: 0.8478\n",
      "Epoch 97/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4220 - acc: 0.8506\n",
      "Epoch 98/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4147 - acc: 0.8530\n",
      "Epoch 99/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4088 - acc: 0.8557\n",
      "Epoch 100/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4020 - acc: 0.8575\n",
      "Epoch 101/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3965 - acc: 0.8591\n",
      "Epoch 102/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3897 - acc: 0.8617\n",
      "Epoch 103/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3848 - acc: 0.8635\n",
      "Epoch 104/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3795 - acc: 0.8653\n",
      "Epoch 105/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3734 - acc: 0.8675\n",
      "Epoch 106/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3679 - acc: 0.8690\n",
      "Epoch 107/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3627 - acc: 0.8709\n",
      "Epoch 108/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3584 - acc: 0.8726\n",
      "Epoch 109/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3535 - acc: 0.8746\n",
      "Epoch 110/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3479 - acc: 0.8767\n",
      "Epoch 111/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3441 - acc: 0.8781\n",
      "Epoch 112/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3374 - acc: 0.8798\n",
      "Epoch 113/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3330 - acc: 0.8818\n",
      "Epoch 114/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3298 - acc: 0.8829\n",
      "Epoch 115/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3254 - acc: 0.8837\n",
      "Epoch 116/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3218 - acc: 0.8853\n",
      "Epoch 117/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3160 - acc: 0.8876\n",
      "Epoch 118/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3135 - acc: 0.8884\n",
      "Epoch 119/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3081 - acc: 0.8905\n",
      "Epoch 120/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3049 - acc: 0.8916\n",
      "Epoch 121/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3006 - acc: 0.8931\n",
      "Epoch 122/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2964 - acc: 0.8948\n",
      "Epoch 123/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2908 - acc: 0.8969\n",
      "Epoch 124/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2883 - acc: 0.8973\n",
      "Epoch 125/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2846 - acc: 0.8990\n",
      "Epoch 126/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2806 - acc: 0.8994\n",
      "Epoch 127/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2757 - acc: 0.9020\n",
      "Epoch 128/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2759 - acc: 0.9021\n",
      "Epoch 129/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2707 - acc: 0.9038\n",
      "Epoch 130/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2666 - acc: 0.9058\n",
      "Epoch 131/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2644 - acc: 0.9056\n",
      "Epoch 132/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2621 - acc: 0.9071\n",
      "Epoch 133/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2581 - acc: 0.9084\n",
      "Epoch 134/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2552 - acc: 0.9094\n",
      "Epoch 135/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2531 - acc: 0.9104\n",
      "Epoch 136/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2498 - acc: 0.9112\n",
      "Epoch 137/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2464 - acc: 0.9124\n",
      "Epoch 138/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2431 - acc: 0.9139\n",
      "Epoch 139/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2422 - acc: 0.9143\n",
      "Epoch 140/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2379 - acc: 0.9152\n",
      "Epoch 141/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2342 - acc: 0.9166\n",
      "Epoch 142/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2328 - acc: 0.9174\n",
      "Epoch 143/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2291 - acc: 0.9188\n",
      "Epoch 144/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2284 - acc: 0.9188\n",
      "Epoch 145/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2258 - acc: 0.9202\n",
      "Epoch 146/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2235 - acc: 0.9205\n",
      "Epoch 147/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2212 - acc: 0.9216\n",
      "Epoch 148/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2193 - acc: 0.9223\n",
      "Epoch 149/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2166 - acc: 0.9234\n",
      "Epoch 150/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2148 - acc: 0.9238\n",
      "Epoch 151/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2120 - acc: 0.9247\n",
      "Epoch 152/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2094 - acc: 0.9262\n",
      "Epoch 153/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2077 - acc: 0.9268\n",
      "Epoch 154/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2048 - acc: 0.9273\n",
      "Epoch 155/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2034 - acc: 0.9276\n",
      "Epoch 156/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2008 - acc: 0.9292\n",
      "Epoch 157/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1995 - acc: 0.9287\n",
      "Epoch 158/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1972 - acc: 0.9305\n",
      "Epoch 159/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1959 - acc: 0.9300\n",
      "Epoch 160/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1926 - acc: 0.9322\n",
      "Epoch 161/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1926 - acc: 0.9318\n",
      "Epoch 162/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1903 - acc: 0.9325\n",
      "Epoch 163/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1887 - acc: 0.9338\n",
      "Epoch 164/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1877 - acc: 0.9339\n",
      "Epoch 165/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1848 - acc: 0.9346\n",
      "Epoch 166/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1836 - acc: 0.9352\n",
      "Epoch 167/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1811 - acc: 0.9358\n",
      "Epoch 168/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1791 - acc: 0.9368\n",
      "Epoch 169/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1781 - acc: 0.9367\n",
      "Epoch 170/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1761 - acc: 0.9380\n",
      "Epoch 171/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1755 - acc: 0.9382\n",
      "Epoch 172/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1728 - acc: 0.9388\n",
      "Epoch 173/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1715 - acc: 0.9395\n",
      "Epoch 174/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1710 - acc: 0.9392\n",
      "Epoch 175/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1692 - acc: 0.9404\n",
      "Epoch 176/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1673 - acc: 0.9405\n",
      "Epoch 177/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1679 - acc: 0.9407\n",
      "Epoch 178/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1647 - acc: 0.9421\n",
      "Epoch 179/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1633 - acc: 0.9424\n",
      "Epoch 180/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1626 - acc: 0.9427\n",
      "Epoch 181/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1620 - acc: 0.9428\n",
      "Epoch 182/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1595 - acc: 0.9441\n",
      "Epoch 183/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1599 - acc: 0.9431\n",
      "Epoch 184/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1568 - acc: 0.9452\n",
      "Epoch 185/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1551 - acc: 0.9451\n",
      "Epoch 186/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1540 - acc: 0.9460\n",
      "Epoch 187/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1535 - acc: 0.9460\n",
      "Epoch 188/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1530 - acc: 0.9459\n",
      "Epoch 189/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1510 - acc: 0.9469\n",
      "Epoch 190/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1496 - acc: 0.9473\n",
      "Epoch 191/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1486 - acc: 0.9477\n",
      "Epoch 192/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1466 - acc: 0.9482\n",
      "Epoch 193/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1455 - acc: 0.9489\n",
      "Epoch 194/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1453 - acc: 0.9490\n",
      "Epoch 195/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1453 - acc: 0.9490\n",
      "Epoch 196/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1421 - acc: 0.9501\n",
      "Epoch 197/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1414 - acc: 0.9503\n",
      "Epoch 198/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1417 - acc: 0.9500\n",
      "Epoch 199/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1402 - acc: 0.9504\n",
      "Epoch 200/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1404 - acc: 0.9509\n",
      "---------------------\n",
      "10000/10000 [==============================] - 1s 98us/sample - loss: 0.0074 - acc: 0.9998\n",
      "============================\n",
      "Cross-validation fold 5\n",
      "Epoch 1/200\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 2.2817 - acc: 0.1302\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 2.1161 - acc: 0.2212\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.9445 - acc: 0.2902\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.8074 - acc: 0.3453\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.7196 - acc: 0.3769\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.6637 - acc: 0.3962\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.6205 - acc: 0.4130\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.5873 - acc: 0.4247\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.5560 - acc: 0.4375\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.5284 - acc: 0.4487\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.5040 - acc: 0.4581\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.4809 - acc: 0.4657\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4589 - acc: 0.4756\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.4351 - acc: 0.4840\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.4135 - acc: 0.4931\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.3927 - acc: 0.5012\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3688 - acc: 0.5104\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3478 - acc: 0.5181\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3276 - acc: 0.5243\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.3062 - acc: 0.5345\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2834 - acc: 0.5415\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.2653 - acc: 0.5492\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2452 - acc: 0.5567\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2277 - acc: 0.5621\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.2110 - acc: 0.5689\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1909 - acc: 0.5767\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1758 - acc: 0.5825\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1589 - acc: 0.5891\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.1415 - acc: 0.5955\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.1265 - acc: 0.6019\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.1109 - acc: 0.6081\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0971 - acc: 0.6126\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0798 - acc: 0.6209\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0662 - acc: 0.6247\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0537 - acc: 0.6293\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0363 - acc: 0.6358\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0242 - acc: 0.6404\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0090 - acc: 0.6456\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.9967 - acc: 0.6499\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.9828 - acc: 0.6552\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9665 - acc: 0.6610\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9550 - acc: 0.6663\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.9409 - acc: 0.6706\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9294 - acc: 0.6741\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9166 - acc: 0.6790\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8998 - acc: 0.6849\n",
      "Epoch 47/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8883 - acc: 0.6896\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8769 - acc: 0.6936\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8614 - acc: 0.6984\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8490 - acc: 0.7035\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8361 - acc: 0.7082\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.8250 - acc: 0.7112\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.8130 - acc: 0.7155\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.7994 - acc: 0.7202\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.7882 - acc: 0.7256\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7771 - acc: 0.7290\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7633 - acc: 0.7332\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.7547 - acc: 0.7367\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7424 - acc: 0.7405\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7308 - acc: 0.7439\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7198 - acc: 0.7488\n",
      "Epoch 62/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.7081 - acc: 0.7521\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6978 - acc: 0.7553\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.6879 - acc: 0.7596\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.6776 - acc: 0.7628\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6674 - acc: 0.7659\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6567 - acc: 0.7697\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.6482 - acc: 0.7724\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6396 - acc: 0.7747\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6280 - acc: 0.7798\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6173 - acc: 0.7834\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.6087 - acc: 0.7862\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5974 - acc: 0.7898\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5909 - acc: 0.7926\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5798 - acc: 0.7957\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.5715 - acc: 0.7987\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5627 - acc: 0.8021\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.5552 - acc: 0.8045\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5445 - acc: 0.8083\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5373 - acc: 0.8107\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5276 - acc: 0.8137\n",
      "Epoch 82/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5206 - acc: 0.8169\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.5122 - acc: 0.8196\n",
      "Epoch 84/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.5038 - acc: 0.8220\n",
      "Epoch 85/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4965 - acc: 0.8251\n",
      "Epoch 86/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4869 - acc: 0.8279\n",
      "Epoch 87/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4813 - acc: 0.8293\n",
      "Epoch 88/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4717 - acc: 0.8330\n",
      "Epoch 89/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4655 - acc: 0.8349\n",
      "Epoch 90/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.4590 - acc: 0.8370\n",
      "Epoch 91/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4515 - acc: 0.8405\n",
      "Epoch 92/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4442 - acc: 0.8424\n",
      "Epoch 93/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.4369 - acc: 0.8449\n",
      "Epoch 94/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.4305 - acc: 0.8472\n",
      "Epoch 95/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.4237 - acc: 0.8493\n",
      "Epoch 96/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4198 - acc: 0.8512\n",
      "Epoch 97/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4127 - acc: 0.8535\n",
      "Epoch 98/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.4065 - acc: 0.8556\n",
      "Epoch 99/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.4004 - acc: 0.8579\n",
      "Epoch 100/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3933 - acc: 0.8599\n",
      "Epoch 101/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3876 - acc: 0.8623\n",
      "Epoch 102/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3818 - acc: 0.8646\n",
      "Epoch 103/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3760 - acc: 0.8669\n",
      "Epoch 104/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3732 - acc: 0.8676\n",
      "Epoch 105/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3666 - acc: 0.8693\n",
      "Epoch 106/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3606 - acc: 0.8726\n",
      "Epoch 107/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3550 - acc: 0.8737\n",
      "Epoch 108/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3508 - acc: 0.8755\n",
      "Epoch 109/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3458 - acc: 0.8765\n",
      "Epoch 110/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3392 - acc: 0.8793\n",
      "Epoch 111/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.3352 - acc: 0.8803\n",
      "Epoch 112/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3308 - acc: 0.8824\n",
      "Epoch 113/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3260 - acc: 0.8841\n",
      "Epoch 114/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.3240 - acc: 0.8849\n",
      "Epoch 115/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3169 - acc: 0.8877\n",
      "Epoch 116/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3113 - acc: 0.8892\n",
      "Epoch 117/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3096 - acc: 0.8893\n",
      "Epoch 118/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.3053 - acc: 0.8917\n",
      "Epoch 119/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2996 - acc: 0.8935\n",
      "Epoch 120/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2968 - acc: 0.8946\n",
      "Epoch 121/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2928 - acc: 0.8961\n",
      "Epoch 122/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2898 - acc: 0.8964\n",
      "Epoch 123/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2872 - acc: 0.8977\n",
      "Epoch 124/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2824 - acc: 0.8995\n",
      "Epoch 125/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2779 - acc: 0.9004\n",
      "Epoch 126/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2758 - acc: 0.9020\n",
      "Epoch 127/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2727 - acc: 0.9025\n",
      "Epoch 128/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2686 - acc: 0.9044\n",
      "Epoch 129/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2665 - acc: 0.9049\n",
      "Epoch 130/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2633 - acc: 0.9065\n",
      "Epoch 131/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2589 - acc: 0.9083\n",
      "Epoch 132/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2564 - acc: 0.9091\n",
      "Epoch 133/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2512 - acc: 0.9107\n",
      "Epoch 134/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2505 - acc: 0.9110\n",
      "Epoch 135/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2468 - acc: 0.9117\n",
      "Epoch 136/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2445 - acc: 0.9130\n",
      "Epoch 137/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2418 - acc: 0.9137\n",
      "Epoch 138/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2383 - acc: 0.9154\n",
      "Epoch 139/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2372 - acc: 0.9150\n",
      "Epoch 140/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2330 - acc: 0.9171\n",
      "Epoch 141/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2300 - acc: 0.9182\n",
      "Epoch 142/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2281 - acc: 0.9187\n",
      "Epoch 143/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2246 - acc: 0.9204\n",
      "Epoch 144/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.2248 - acc: 0.9203\n",
      "Epoch 145/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2205 - acc: 0.9219\n",
      "Epoch 146/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2186 - acc: 0.9223\n",
      "Epoch 147/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2173 - acc: 0.9226\n",
      "Epoch 148/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2151 - acc: 0.9239\n",
      "Epoch 149/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2127 - acc: 0.9244\n",
      "Epoch 150/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.2117 - acc: 0.9243\n",
      "Epoch 151/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2068 - acc: 0.9268\n",
      "Epoch 152/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.2048 - acc: 0.9277\n",
      "Epoch 153/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2019 - acc: 0.9286\n",
      "Epoch 154/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.2026 - acc: 0.9278\n",
      "Epoch 155/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1993 - acc: 0.9293\n",
      "Epoch 156/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1990 - acc: 0.9296\n",
      "Epoch 157/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1961 - acc: 0.9305\n",
      "Epoch 158/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1938 - acc: 0.9316\n",
      "Epoch 159/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1935 - acc: 0.9320\n",
      "Epoch 160/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1893 - acc: 0.9330\n",
      "Epoch 161/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1874 - acc: 0.9337\n",
      "Epoch 162/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1869 - acc: 0.9336\n",
      "Epoch 163/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1851 - acc: 0.9347\n",
      "Epoch 164/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1828 - acc: 0.9354\n",
      "Epoch 165/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1821 - acc: 0.9354\n",
      "Epoch 166/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1796 - acc: 0.9362\n",
      "Epoch 167/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1789 - acc: 0.9366\n",
      "Epoch 168/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1769 - acc: 0.9373\n",
      "Epoch 169/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1750 - acc: 0.9381\n",
      "Epoch 170/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1728 - acc: 0.9389\n",
      "Epoch 171/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1727 - acc: 0.9386\n",
      "Epoch 172/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1707 - acc: 0.9395\n",
      "Epoch 173/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1686 - acc: 0.9404\n",
      "Epoch 174/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1688 - acc: 0.9402\n",
      "Epoch 175/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1652 - acc: 0.9418\n",
      "Epoch 176/200\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.1637 - acc: 0.9423\n",
      "Epoch 177/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1637 - acc: 0.9422\n",
      "Epoch 178/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1626 - acc: 0.9428\n",
      "Epoch 179/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1606 - acc: 0.9434\n",
      "Epoch 180/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1570 - acc: 0.9446\n",
      "Epoch 181/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1581 - acc: 0.9444\n",
      "Epoch 182/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1565 - acc: 0.9450\n",
      "Epoch 183/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1561 - acc: 0.9447\n",
      "Epoch 184/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1530 - acc: 0.9460\n",
      "Epoch 185/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1528 - acc: 0.9462\n",
      "Epoch 186/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1517 - acc: 0.9465\n",
      "Epoch 187/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1509 - acc: 0.9466\n",
      "Epoch 188/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1496 - acc: 0.9475\n",
      "Epoch 189/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1504 - acc: 0.9467\n",
      "Epoch 190/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1469 - acc: 0.9485\n",
      "Epoch 191/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1462 - acc: 0.9487\n",
      "Epoch 192/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1455 - acc: 0.9486\n",
      "Epoch 193/200\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.1428 - acc: 0.9495\n",
      "Epoch 194/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1433 - acc: 0.9498\n",
      "Epoch 195/200\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.1412 - acc: 0.9505\n",
      "Epoch 196/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1413 - acc: 0.9504\n",
      "Epoch 197/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1402 - acc: 0.9505\n",
      "Epoch 198/200\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.1386 - acc: 0.9514\n",
      "Epoch 199/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1362 - acc: 0.9521\n",
      "Epoch 200/200\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.1366 - acc: 0.9517\n",
      "---------------------\n",
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.0067 - acc: 1.0000\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "print(cifar_train_norm.shape)\n",
    "print(cifar_train_labels.shape)\n",
    "\n",
    "epochs = 200\n",
    "steps_per_epoch = 40\n",
    "batch_size = 1024\n",
    "learning_rate = 0.0001\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "for train_indices, test_indices in kfold.split(cifar_train_norm, cifar_train_labels_sparse):\n",
    "  print(\"Cross-validation fold\", i)\n",
    "  i+=1\n",
    "  # get data\n",
    "  cifar_train_norm_part = cifar_train_norm[train_indices]\n",
    "  cifar_train_labels_sparse_part = cifar_train_labels_sparse[train_indices]\n",
    "  cifar_train_labels_part = to_categorical(cifar_train_labels_sparse_part)\n",
    "  \n",
    "  cifar_test_norm_part = cifar_train_norm[test_indices]\n",
    "  cifar_test_labels_sparse_part = cifar_train_labels_sparse[test_indices]\n",
    "  cifar_test_labels_part = to_categorical(cifar_test_labels_sparse_part)\n",
    "  \n",
    "  #print(cifar_train_norm_part.shape)\n",
    "  #print(cifar_train_labels_part.shape)\n",
    "  #print(cifar_test_norm_part.shape)\n",
    "  #print(cifar_test_labels_part.shape)\n",
    "\n",
    "  \n",
    "  def train_input_fn_cv(batch_size=1024):\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((cifar_train_norm_part, cifar_train_labels_part))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "  # reset weights\n",
    "  tpu_model.set_weights(initial_weights)\n",
    "\n",
    "  # regen model\n",
    "  # opt = tf.train.AdamOptimizer(learning_rate)\n",
    "  # model = modelDeeperCNN()\n",
    "  # model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "  # tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "  #   model,\n",
    "  #   strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "  #       tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
    "\n",
    "  # retrain\n",
    "  history = tpu_model.fit(train_input_fn_cv, steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "  print(\"---------------------\")\n",
    "  tpu_model.evaluate(cifar_test_norm_part, cifar_test_labels_part, batch_size=batch_size)\n",
    "  print(\"============================\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "BiAFGIhNDgoq",
    "tsDq3GrzMQoO",
    "HvlNsw21Opul",
    "VFUizZo6MkvV",
    "kYSH_uk0IeY7"
   ],
   "name": "cifar-10-tests.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
